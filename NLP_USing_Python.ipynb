{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Introduction\n",
    "Welcome to the course on NLP Using Python. In this course, you will get to learn the following topics:<br>\n",
    "\n",
    "* Tokenizing text using functions word_tokenize and sent_tokenize.\n",
    "\n",
    "* Computing Frequencies with FreqDist and ConditionalFreqDist.\n",
    "\n",
    "* Generating Bigrams and collocations with bigrams and collocations.\n",
    "\n",
    "* Stemming word affixes using PorterStemmer and LancasterStemmer.\n",
    "\n",
    "* Tagging words to their parts of speech using pos_tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Natural Language Processing\n",
    "* Humans communicate in natural languages such as English, German, Japanese and so on.\n",
    "\n",
    "* On the other hand, a Computer communicates in Machine Language, which has a defined set of rules.\n",
    "\n",
    "* As a reason, a computer cannot communicate with humans in an effective way\n",
    "\n",
    "* Natural Language Processing helps in increasing computer intelligence to understand human languages as spoken and to respond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why NLP?\n",
    "* NLP techniques are capable of processing and extracting meaningful insights, from huge unstructured data available online.\n",
    "\n",
    "* It can automate translating text from one language to other.\n",
    "\n",
    "* These techniques can be used for performing sentiment analysis.\n",
    "\n",
    "* It helps in building applications that interact with humans as humans do.\n",
    "\n",
    "* Also, NLP can help in automating Text Classification, Spam Filtering, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk\n",
    "* nltk is a popular Python framework used for developing Python programs to work with human language data.<br>\n",
    "Key features of nltk:<br>\n",
    "* It provides access to over 50 text corpora and other lexical resources.\n",
    "* It is a suite of text processing tools.\n",
    "* It is free to use and Open source.\n",
    "* It is available for Windows, Mac OS X, and Linux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Understanding of nltk\n",
    "Now let's understand by performing simple tasks in the next couple of slides.<br>\n",
    "\n",
    "__Note__: nltk.download('punkt') after the mentioned line below\n",
    "* Splitting a sample text into a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"Python is an interpreted high-level programming language for general-purpose programming. Created by Guido van Rossum and first released in 1991.\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__As seen above, sent_tokenize function generates sentences from the given text.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Splitting a sample text into words using __word_tokenize__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python', 'is', 'an', 'interpreted', 'high-level']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(text)\n",
    "len(words)\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The expression words[:5] displays first five words of list words.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Determining the frequency of words present in sample text using FreqDist function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('programming', 2), ('.', 2)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordfreq = nltk.FreqDist(words)\n",
    "wordfreq.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The expression wordfreq.most_common(2) displays two highly frequent words with their respective frequency count.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading NLTK Book collection\n",
    "* In this course, you will be coordinating with several texts curated by NLTK authors.\n",
    "* These texts are available in collection book of nltk.\n",
    "* They can be downloaded by running the following command in Python interpreter, after importing nltk successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Items of 'book'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above figure illustrates the output of the command from nltk.book import *.\n",
    "* The command loads nine texts and nine sentences, from the collection book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching Text\n",
    "* There are multiple ways of searching for a pattern in a text.\n",
    "* The example shown below searches for words starting with tri, and ending with r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "triangular; triangular; triangular; triangular\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "text1.findall(\"<tri.*r>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tasks with Text\n",
    "__In this topic, you will understand how to perform the following activities, using text1 as input text.__<br>\n",
    "* Total Word Count\n",
    "* Unique Word Count\n",
    "* Transforming Words\n",
    "* Word Coverage\n",
    "* Filtering Words\n",
    "* Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Total Word Count\n",
    "* NLTK Book module consits of 9 text class. Type the name of the text or sentence to view it.\n",
    "* * * text1: Moby Dick by Herman Melville 1851\n",
    "* * * text2: Sense and Sensibility by Jane Austen 1811\n",
    "* * * text3: The Book of Genesis\n",
    "* * * text4: Inaugural Address Corpus\n",
    "* * * text5: Chat Corpus\n",
    "* * * text6: Monty Python and the Holy Grail\n",
    "* * * text7: Wall Street Journal\n",
    "* * * text8: Personals Corpus\n",
    "* * * text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
    "* The text1, imported from nltk.book is an object of nltk.text.Text class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "type(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total number of words in text1 is determined using len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = len(text1)\n",
    "n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Unique Word Count\n",
    "* A unique number of words in text1 is determined using set and len methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2166"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_unique_words = len(set(text6))\n",
    "n_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__set(text1) generates list of unique words from text1.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Words\n",
    "* It is possible to apply a function to any number of words and transform them.* *\n",
    "* Now let's transform every word of text1 to lowercase and determine unique words once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17231"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1_lcw = [ word.lower() for word in set(text1) ]\n",
    "n_unique_words_lc = len(set(text1_lcw))\n",
    "n_unique_words_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A difference of 2086 can be found from n_unique_words.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Word Coverage\n",
    "Word Coverage: Word Coverage refers to an average number of times a word is occurring in the text.<br>\n",
    "* The following examples determine Word Coverage of raw and transformed text1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.502044830977896"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coverage1 = n_words / n_unique_words\n",
    "word_coverage1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On average, a single word in text1 is repeated 13.5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.136614241773549"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coverage2 = n_words / n_unique_words_lc\n",
    "word_coverage2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Words\n",
    "* Now let's see how to filter words based on specific criteria.\n",
    "* The following example filters words having characters more than 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uninterpenetratingly', 'characteristically']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_words = [word for word in set(text1) if len(word) > 17 ]\n",
    "big_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A list of comprehension with a condition is used above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Words\n",
    "* Now let's see one more example which filters words having the prefix Sun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sunset', 'Sunday', 'Sunda']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sun_words = [word for word in set(text1) if word.startswith('Sun') ]\n",
    "sun_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above example is case-sensitive. It doesn't filter the words starting with lowercase s and followed by un."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution\n",
    "* FreqDist functionality of nltk can be used to determine the frequency of all words, present in an input text.\n",
    "* The following example, determines frequency distribution of text1 and further displays the frequency of word Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1_freq = nltk.FreqDist(text6)\n",
    "text1_freq['ARTHUR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Methods of Frequency Distribution\n",
    "* Illustration of Commonly used methods on a frequency distribution fdist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now let's identify three frequent words from text1_freq distribution using most_common method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18713), ('the', 13721), ('.', 6862)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top3_text1 = text1_freq.most_common(3)\n",
    "top3_text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The output says the three most frequent words are , , the, and .\n",
    "* It may be weird for few of you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution\n",
    "In general, you would be interested in finding frequent words which are not common in usage and specific to input text.<br>\n",
    "In the next example, you will perform the following.\n",
    "* Filter words having all characters and of larger length.\n",
    "* Determine frequency distribution of the filtered words.\n",
    "* Identify the three most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Queequeg', 252),\n",
       " ('Starbuck', 196),\n",
       " ('something', 119),\n",
       " ('Nantucket', 85),\n",
       " ('sometimes', 81),\n",
       " ('harpooneer', 77),\n",
       " ('standing', 73),\n",
       " ('whalemen', 71),\n",
       " ('business', 67),\n",
       " ('Leviathan', 64)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_uncommon_words = [word for word in text1 if word.isalpha() and len(word) > 7 ]\n",
    "text1_uncommon_freq = nltk.FreqDist(large_uncommon_words)\n",
    "text1_uncommon_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Queequeg, Starbuck and something are the top three words, based on the chosen criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Handson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python3\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import zipfile\n",
    "\n",
    "os.environ['NLTK_DATA'] = os.getcwd()+\"/nltk_data\"\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.text import Text\n",
    "\n",
    "#\n",
    "# Complete the 'calculateWordCounts' function below.\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "def calculateWordCounts(text):\n",
    "    # Write your code here\n",
    "    n_words = len(text)\n",
    "    print(n_words)\n",
    "    n_unique_words = len(set(text))\n",
    "    print(n_unique_words)\n",
    "    word_coverage1 = n_words / n_unique_words\n",
    "    print(int(word_coverage1//1))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = input()\n",
    "    if not os.path.exists(os.getcwd()+\"/nltk_data\"):\n",
    "        with zipfile.ZipFile(\"nltk_data.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.getcwd())\n",
    "    \n",
    "    text = Text(gutenberg.words(text))\n",
    "\n",
    "    calculateWordCounts(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python3\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "os.environ['NLTK_DATA'] = os.getcwd() + \"/nltk_data\"\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.text import Text\n",
    "\n",
    "\n",
    "#\n",
    "# Complete the 'filterWords' function below.\n",
    "#\n",
    "# \n",
    "#\n",
    "\n",
    "def filterWords(text):\n",
    "    # Write your code here\n",
    "    ing_words = [word for word in set(text) if word.endswith('ing') ]\n",
    "    large_words = [word for word in text if len(word) > 15 ]\n",
    "    upper_words = [word for word in set(text) if word.isupper() ]\n",
    "    return ing_words,large_words,upper_words\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    text = input()\n",
    "    if not os.path.exists(os.getcwd() + \"/nltk_data\"):\n",
    "        with zipfile.ZipFile(\"nltk_data.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.getcwd())\n",
    "            \n",
    "    text = Text(gutenberg.words(text))\n",
    "\n",
    "    ing_words, big_words, upper_words = filterWords(text)\n",
    "\n",
    "    print(sorted(ing_words))\n",
    "    print(sorted(big_words))\n",
    "    print(sorted(upper_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python3\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import zipfile\n",
    "os.environ['NLTK_DATA'] = os.getcwd() + \"/nltk_data\"\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.text import Text\n",
    "import nltk\n",
    "\n",
    "\n",
    "#\n",
    "# Complete the 'findWordFreq' function below.\n",
    "#\n",
    "# \n",
    "\n",
    "def findWordFreq(text, word):\n",
    "    # Write your code here\n",
    "    text1_freq = nltk.FreqDist(text)\n",
    "    wordfreq=text1_freq[word]\n",
    "    large_uncommon_words = [word for word in text if word.isalpha()]\n",
    "    text_uncommon_freq = nltk.FreqDist(large_uncommon_words)\n",
    "    maxfreq=text_uncommon_freq.most_common(1)\n",
    "    maxfreq=maxfreq[0][0]\n",
    "    return wordfreq,maxfreq\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = input()\n",
    "    word = input()\n",
    "    if not os.path.exists(os.getcwd() + \"/nltk_data\"):\n",
    "        with zipfile.ZipFile(\"nltk_data.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.getcwd())\n",
    "\n",
    "    text = Text(gutenberg.words(text))\n",
    "\n",
    "    word_freq, max_freq = findWordFreq(text, word)\n",
    "\n",
    "    print(word_freq)\n",
    "    print(max_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Corpora\n",
    "* In the previous topic, you have worked with a simple text collection text1.\n",
    "* In this topic, you will work with larger text collections known as Text Corpora or Text Corpus.\n",
    "* The following code snippet downloads more text corpus, which is varied in content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\abc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular Text Corpora\n",
    "## Two popular __Text Corpora__ available from __nltk__, which you will be using in this course are:\n",
    "* __Genesis__: It is a collection of few words across multiple languages.\n",
    "* __Brown__: It is the first electronic corpus of one million English words.\n",
    "## Other Corpus in nltk\n",
    "* __Gutenberg__ : Collections from Project Gutenberg\n",
    "* __Inaugural___ : Collection of U.S Presidents inaugural speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular Text Corpora\n",
    "* __stopwords__ : Collection of stop words.\n",
    "* __reuters__ : Collection of news articles.\n",
    "* __cmudict__ : Collection of CMU Dictionary words.\n",
    "* __movie_reviews__ : Collection of Movie Reviews.\n",
    "* __np_chat__ : Collection of chat text.\n",
    "* __names__ : Collection of names associated with males and females.\n",
    "* __state_union__ : Collection of state union address.\n",
    "* __wordnet__ : Collection of all lexical entries.\n",
    "* __words__ : Collection of words in Wordlist corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Text Corpora\n",
    "* Any text corpus has to be imported before you start working with it.\n",
    "* The below code imports __genesis__ text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import genesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Various text collections available under genesis text corpus are viewed by fileids method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['english-kjv.txt',\n",
       " 'english-web.txt',\n",
       " 'finnish.txt',\n",
       " 'french.txt',\n",
       " 'german.txt',\n",
       " 'lolcat.txt',\n",
       " 'portuguese.txt',\n",
       " 'swedish.txt']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genesis.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The output displays eight text collections, present in genesis corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with a Text Corpus\n",
    "* Now let's understand how to work with a text corpus.\n",
    "* The following example determines the average word length and average sentence length of each text collection present in genesis corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 30 english-kjv.txt\n",
      "4 19 english-web.txt\n",
      "5 15 finnish.txt\n",
      "4 23 french.txt\n",
      "4 23 german.txt\n",
      "4 20 lolcat.txt\n",
      "4 27 portuguese.txt\n",
      "4 30 swedish.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in genesis.fileids():\n",
    "    n_chars = len(genesis.raw(fileid))\n",
    "    n_words = len(genesis.words(fileid))\n",
    "    n_sents = len(genesis.sents(fileid))\n",
    "    print(int(n_chars/n_words), int(n_words/n_sents), fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The methods raw, words and sents used in code determine the total number of characters, words, and sentences present in a specific text collection.\n",
    "* The output of code is shown below. Text collection finnish.txt has different average word length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Corpus Structure\n",
    "A text corpus is organized into any of the following four structures.\n",
    "* __Isolated__ - Holds Individual text collections.\n",
    "* __Categorized__ - Each text collection tagged to a category.\n",
    "* __Overlapping__ - Each text collection tagged to one or more categories, and\n",
    "* __Temporal__ - Each text collection tagged to a period, date, time, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolated Text Corpus\n",
    "* genesis text corpus has eight text collections, which are isolated in structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorized Text Corpus\n",
    "* Each text collection is tagged to a specific category or genre.\n",
    "* E.g.: Brown text corpus contains 500 collections, which are categorized into 15 genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlapping Text Corpus\n",
    "* Each collection is categorized into one or more genre.\n",
    "* E.g.: Reuters corpus contains 10788 collections, which are tagged to 90 genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Text Corpus\n",
    "* Each text collection is tagged to a period of time.\n",
    "* E.g.: inaugural corpus contain text collections corresponding to U.S inaugural presidential speeches, gathered over a period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading User Specific Corpus\n",
    "* Now let's see how to convert your collection of text files into a text corpus.\n",
    "* Suppose, you have three files c1.txt, c2.txt and c3.txt in /usr/home/dict path.\n",
    "* Creation of corpus wordlists corpus is shown in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = '/usr/share/dict'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "wordlists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python3\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import zipfile\n",
    "os.environ['NLTK_DATA'] = os.getcwd() + \"/nltk_data\"\n",
    "import nltk\n",
    "\n",
    "\n",
    "#\n",
    "# Complete the 'accessTextCorpora' function below.\n",
    "#\n",
    "# The function accepts following parameters:\n",
    "#  1. STRING fileid\n",
    "#  2. STRING word\n",
    "#\n",
    "\n",
    "def accessTextCorpora(fileid, word):\n",
    "    # Write your code here\n",
    "    from nltk.corpus import inaugural\n",
    "    file_words = inaugural.words(fileid)\n",
    "    wordcoverage = int(len(file_words)/len(set(file_words)))\n",
    "    ed_words = [words for words in set(file_words) if words.endswith('ed')]\n",
    "    textfreq2 = [word.lower() for word in file_words if word.isalpha()]\n",
    "    textfreq = nltk.FreqDist(textfreq2)\n",
    "    wordfreq = textfreq[word]\n",
    "    \n",
    "    return wordcoverage, ed_words, wordfreq\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fileid = input()\n",
    "    word = input()\n",
    "\n",
    "    if not os.path.exists(os.getcwd() + \"/nltk_data\"):\n",
    "        with zipfile.ZipFile(\"nltk_data.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.getcwd())\n",
    "\n",
    "    word_coverage, ed_words, word_freq = accessTextCorpora(fileid, word)\n",
    "\n",
    "    print(word_coverage)\n",
    "    print(sorted(ed_words))\n",
    "    print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python3\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "#\n",
    "# Complete the 'createUserTextCorpora' function below.\n",
    "#\n",
    "# The function accepts following parameters:\n",
    "#  1. STRING filecontent1\n",
    "#  2. STRING filecontent2\n",
    "#\n",
    "\n",
    "def createUserTextCorpora(filecontent1, filecontent2):\n",
    "    # Write your code here\n",
    "    with open(os.path.join('nltk_data/','content1.txt'),\"w\") as file1:\n",
    "            file1.write(filecontent1)\n",
    "            file1.close()\n",
    "    with open(os.path.join('nltk_data/','content2.txt'),\"w\") as file2:\n",
    "        file2.write(filecontent2)\n",
    "        file2.close()\n",
    "    \n",
    "    text_corpus = PlaintextCorpusReader('nltk_data/','.*')\n",
    "    no_of_words_corpus1 = len(text_corpus.words('content1.txt'))\n",
    "    no_of_words_corpus2 = len(text_corpus.words('content2.txt'))\n",
    "    no_of_unique_words_corpus1 = len(set(text_corpus.words('content1.txt')))\n",
    "    no_of_unique_words_corpus2 = len(set(text_corpus.words('content2.txt')))\n",
    "    return text_corpus,no_of_words_corpus1,no_of_unique_words_corpus1,no_of_words_corpus2,no_of_unique_words_corpus2\n",
    "if __name__ == '__main__':\n",
    "    filecontent1 = input()\n",
    "\n",
    "    filecontent2 = input()\n",
    "\n",
    "    path = os.path.join(os.getcwd(), \"nltk_data\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    for file in os.listdir(path):\n",
    "        os.remove(path+\"\\\\\"+file)\n",
    "\n",
    "\n",
    "    text_corpus, no_of_words_corpus1, no_of_unique_words_corpus1, no_of_words_corpus2, no_of_unique_words_corpus2 = createUserTextCorpora(filecontent1, filecontent2)\n",
    "    expected_corpus_files = ['content1.txt', 'content2.txt']\n",
    "    if type(text_corpus) == nltk.corpus.reader.plaintext.PlaintextCorpusReader and sorted(list(text_corpus.fileids())) == expected_corpus_files:\n",
    "        print(no_of_words_corpus1)\n",
    "        print(no_of_unique_words_corpus1)\n",
    "        print(no_of_words_corpus2)\n",
    "        print(no_of_unique_words_corpus2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
