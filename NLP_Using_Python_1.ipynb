{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Frequency\n",
    "* In the previous topic, you have studied about Frequency Distributions.\n",
    "* FreqDist function computes the frequency of each item in a list.\n",
    "* While computing a frequency distribution, you observe occurrence count of an event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'apple': 2, 'cabbage': 2, 'kiwi': 1, 'potato': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "items = ['apple', 'apple', 'kiwi', 'cabbage', 'cabbage', 'potato']\n",
    "nltk.FreqDist(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Conditional Frequency is a collection of frequency distributions, computed based on a condition.\n",
    "* For computing a conditional frequency, you have to attach a condition to every occurrence of an event.\n",
    "* Let's consider the following list for computing Conditional Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_items = [('F','apple'), ('F','apple'), ('F','kiwi'), ('V','cabbage'), ('V','cabbage'), ('V','potato') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each item is grouped either as a fruit F or a vegetable V."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Conditional Frequency\n",
    "* ConditionalFreqDist function of nltk is used to compute Conditional Frequency Distribution (CDF).\n",
    "* The same can be viewed in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 2 samples and 3 outcomes>\n",
      "<FreqDist with 2 samples and 3 outcomes>\n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(c_items)\n",
    "cfd.conditions()\n",
    "print(cfd['V'])\n",
    "print(cfd['F'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common methods of a CFD\n",
    "* Illustration of Commonly used methods on a conditional frequency distribution, cfdist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Words by Genre\n",
    "* Now let's determine the frequency of words, of a particular genre, in __brown corpus.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist([ (genre, word) for genre in brown.categories() for word in brown.words(categories=genre) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The conditions applied can be viewed as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing Word Count\n",
    "* Once after computing conditional frequency distribution, __tabulate__ method is used for viewing the count along with arguments __conditions__ and __samples__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing Cumulative Word Count\n",
    "* The cumulative count for different conditions is found by setting __cumulative__ argument value to __True__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'], cumulative = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Individual Frequency Distributions\n",
    "* From the obtained conditional frequency distribution, you can access individual frequency distributions.\n",
    "* The below example extracts frequency distribution of words present in __news__ genre of __brown__ corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_fd = cfd['news']\n",
    "news_fd.most_common(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can further access count of any sample as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_fd['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Frequency Distributions\n",
    "* Now let's see another example, which computes the frequency of last character appearing in all names associated with males and females respectively and compares them.\n",
    "* The text corpus __names__ contain two files __male.txt__ and __female.txt__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "nt = [(fid.split('.')[0], name[-1])    for fid in names.fileids() for name in names.words(fid) ]\n",
    "cfd2 = nltk.ConditionalFreqDist(nt)\n",
    "cfd2['female'] > cfd2['male']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The expression __cfd2['female'] > cfd2['male']__ checks if the last characters in females occur more frequently than the last characters in males."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Frequency Distributions\n",
    "* The following code snippet displays frequency count of characters __a__ and __e__ in __females__ and __males__, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a    e \n",
      "female 1773 1432 \n",
      "  male   29  468 \n"
     ]
    }
   ],
   "source": [
    "cfd2.tabulate(samples=['a', 'e'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can observe a significant difference in frequencies of __a__ and __e__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Handson - NLP - Python - Conditional Frequency DistributionNLP - Python - Conditional Frequency Distribution Define a function called `calculateCFD`, which takes two parameters as a list. The first parameter, `cfdconditions`, is a list of categories, and the second parameter, `cfdevents`, is a list of samples. The function definition code stub is given in the editor. Perform the given operation for the given `cfdconditions` and `cfdevents`: Determine the conditional frequency of all the words (convert into lower case and remove all the stop words) for the given category `cfdconditions` of the brown corpora. Store the result in cdev_cfd. (Hint: Use stopwords corpora to identify the stop words.)Determine the words ending with `ing` or `ed`. Compute conditional frequency distribution, where the condition is `cfdconditions`, and the event is either `ing` or `ed`. Store the conditional frequency distribution in the variable inged_cfd.Compute the condition frequency using the condition `cfdconditions` and event `cfdevents`. Display the frequency of modal in the form of a table. (Hint: Use the tabulate method)Compute the condition frequency using the condition `cfdconditions` and event ['ing', 'ed']. Display the frequency of modal in the form of a table. (Hint: Use the tabulate method) Input Format for Custom TestingInput from stdin will be processed as follows and passed to the function.The first line contains an integer m, the size of the cfdconditions.Each of the next m lines contains cfdconditions[i] where 0 ≤ i < m.The next line contains an integer n, the size of the cfdevents.Each of the next n lines contains an cfdevents[i] where 0 ≤ i < n. Sample Case Sample InputSTDIN Function Parameters ----- ------------------- 2 → cfdconditions[] Size m = 2government → cfdconditions[] = ['government', 'hobbies'] hobbies 2 → cfdevents[] Size n = 2 first → cfdevents[] = ['first', 'last'] lastSample Output first last government 62 21 hobbies 126 26 ed ing government 2507 1474 hobbies 2561 2169 ExplanationThe first table shows that frequency distribution based on the cfdconditions (['government', 'hobbies']) and cfdevents (['first', 'last']) .The second table shows that frequency distribution based on the cfdconditions (['government', 'hobbies']) and ['ed', 'ing'] ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python3\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import zipfile\n",
    "os.environ['NLTK_DATA'] = os.getcwd()+\"/nltk_data\"\n",
    "import nltk\n",
    "\n",
    "#\n",
    "# Complete the 'calculateCFD' function below.\n",
    "#\n",
    "# The function accepts following parameters:\n",
    "#  1. STRING_ARRAY cfdconditions\n",
    "#  2. STRING_ARRAY cfdevents\n",
    "#\n",
    "\n",
    "def calculateCFD(cfdconditions, cfdevents):\n",
    "    # Write your code here\n",
    "    from nltk.corpus import brown\n",
    "    from nltk import ConditionalFreqDist\n",
    "    from nltk.corpus import stopwords\n",
    "    stopword = set(stopwords.words('english'))\n",
    "    cdev_cfd = nltk.ConditionalFreqDist([(genre, word.lower()) for genre in brown.categories() for word in brown.words(categories=genre) if not word.lower()  in stopword])\n",
    "    cdev_cfd.tabulate(conditions = cfdconditions, samples = cfdevents)\n",
    "    inged_cfd = [ (genre, word.lower()) for genre in brown.categories() for word in brown.words(categories=genre) if (word.lower().endswith('ing') or word.lower().endswith('ed')) ]\n",
    "    inged_cfd = [list(x) for x in inged_cfd]\n",
    "    for wd in inged_cfd:\n",
    "        if wd[1].endswith('ing') and wd[1] not in stopword:\n",
    "            wd[1] = 'ing'\n",
    "        elif wd[1].endswith('ed') and wd[1] not in stopword:\n",
    "            wd[1] = 'ed'\n",
    "    #print(inged_cfd)\n",
    "    inged_cfd = nltk.ConditionalFreqDist(inged_cfd)\n",
    "    #print(inged_cfd.conditions())    \n",
    "    inged_cfd.tabulate(conditions=cfdconditions, samples = ['ed','ing'])\n",
    "if __name__ == '__main__':\n",
    "    cfdconditions_count = int(input().strip())\n",
    "\n",
    "    cfdconditions = []\n",
    "\n",
    "    for _ in range(cfdconditions_count):\n",
    "        cfdconditions_item = input()\n",
    "        cfdconditions.append(cfdconditions_item)\n",
    "\n",
    "    cfdevents_count = int(input().strip())\n",
    "\n",
    "    cfdevents = []\n",
    "\n",
    "    for _ in range(cfdevents_count):\n",
    "        cfdevents_item = input()\n",
    "        cfdevents.append(cfdevents_item)\n",
    "\n",
    "    if not os.path.exists(os.getcwd() + \"/nltk_data\"):\n",
    "        with zipfile.ZipFile(\"nltk_data.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.getcwd())\n",
    "\n",
    "    calculateCFD(cfdconditions, cfdevents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Text Processing\n",
    "* For most of the NLTK studies that you carry out, data is not readily available in the form of a text corpus.\n",
    "* Also, raw text data from a different source can be obtained, processed and used for doing NLTK studies.\n",
    "* Some of the processing steps that you perform are\n",
    "   * Tokenization\n",
    "   * Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a Text File\n",
    "* In this topic, you will understand how data is read from different external sources.\n",
    "* The following example reads content from a text file, available at Project Gutenberg site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "content1 = request.urlopen(url).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a HTML file\n",
    "* The following example reads content from a news article available over the web.\n",
    "* __Beautifulsoup__ module is used for scrapping the required text from the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.bbc.com/news/health-42802191\"\n",
    "html_content = request.urlopen(url).read()\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_body = soup.find_all('div', attrs={'class':'story-body__inner'})\n",
    "inner_text = [elm.text for elm in inner_body[0].find_all(['h1', 'h2', 'p', 'li']) ]\n",
    "text_content2 = '\\n'.join(inner_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* find_all method returns all inner elements of div element, having class attribute value as story-body__inner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from Other Sources\n",
    "* You can also read text from some other text resources such as RSS feeds, FTP repositories, local text files, etc.\n",
    "* It is also possible to read a text in binary format, from sources like Microsoft Word and PDF.\n",
    "* Third party libraries such as __pywin32__, __pypdf__ are required for accessing Microsoft Word or PDF documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "* __Tokenization__ is a step in which a text is broken down into words and punctuation.\n",
    "* The simplest way of tokenizing is by using __word_tokenize__ method.\n",
    "* The below example tokenizes text read from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project', 'Gutenberg', 'eBook', 'of', 'Crime']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_content1 = content1.decode('unicode_escape')  # Converts bytes to unicode\n",
    "tokens1 = nltk.word_tokenize(text_content1)\n",
    "tokens1[3:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following example tokenizes text scrapped from the HTML page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = nltk.word_tokenize(text_content2)\n",
    "tokens2[:5]\n",
    "len(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions for Tokenization\n",
    "* Regular expressions can also be utilized to split the text into tokens.\n",
    "* The below example splits the entire text __text_content2__ with regular expression __\\w+__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2_2 = re.findall(r'\\w+', text_content2)\n",
    "len(tokens2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions for Tokenization\n",
    "* __nltk__ contains the function __regexp_tokenize__, which can be used similarly to __re.findall__ and produce the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\w+'\n",
    "tokens2_3 = nltk.regexp_tokenize(text_content2, pattern)\n",
    "len(tokens2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of NLTK text\n",
    "* Using the obtained list of tokens, an object of NLTK text can be created as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text2 = nltk.Text(tokens2)\n",
    "type(input_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Thus obtained text can be used for further linguistic processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hands-on - NLP - Python - Processing Raw TextNLP - Python - Processing Raw Text Define a function called `processRawText`, which takes a parameter. The first parameter `textURL` is an URL link. The function definition code stub is given in the editor. Perform the following tasks: Read the text content from the given link `textURL`. Store the content in the variable `textcontent`.Tokenize all the words in the `textcontent`, and convert them into lower case. Store the tokenized list of words in `tokenizedlcwords`. (Hint: Use word tokenize)Find the number of words in `tokenizedlcwords`, and store the result in `noofwords`.Find the number of unique words in `tokenizedlcwords`, and store the result in `noofunqwords`.Calculate the word coverage of `tokenizedlcwords` obtained from the number of words and number of unique words, Store the result in the `wordcov`.Determine the frequency distribution of all words having only alphabets in `tokenizedlcwords`. Store the result in the variable 'wordfreq'.Find the maximum frequent word of `tokenizedlcwords`. Store the result in the variable 'maxfreq'. Return `noofwords`, `noofunqwords`, `wordcov`, and `maxfreq` variables from the function. Note: Word coverage should be of the int data type. Input Format for Custom TestingInput from stdin will be processed as follows and passed to the function. The first line contains a string `textURL`, URL of the text content. Sample Case Sample InputSTDIN Function Parameters ----- ------------------- https://hrcdn.net/s3_pub/istreet-assets/2KDELtu3svGwJgNXUXFE7Q/001.txt → textURL = 'https://hrcdn.net/s3_pub/istreet-assets/2KDELtu3svGwJgNXUXFE7Q/001.txt' Sample Output:2101271the ExplanationThe first line denotes the number of words in `tokenizedlcwords`.The second line denotes the unique number of words in `tokenizedlcwords`.The third line denotes the word coverage in `tokenizedlcwords`.The fourth line denotes the most frequent word in `tokenizedlcwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python3\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import zipfile\n",
    "os.environ['NLTK_DATA'] = os.getcwd() + \"/nltk_data\"\n",
    "import nltk\n",
    "\n",
    "#\n",
    "# Complete the 'processRawText' function below.\n",
    "#\n",
    "# The function accepts STRING textURL as parameter.\n",
    "#\n",
    "\n",
    "def processRawText(textURL):\n",
    "    # Write your code here\n",
    "    from urllib import request\n",
    "    content1 = request.urlopen(textURL).read()\n",
    "    text_content1 = content1.decode('unicode_escape')\n",
    "    text_content1 =text_content1\n",
    "    tokenizedlcwords = nltk.word_tokenize(text_content1)\n",
    "    noofwords=len(tokenizedlcwords)\n",
    "    text_content1 =text_content1.lower()\n",
    "    tokenizedlcwords = nltk.word_tokenize(text_content1)\n",
    "    noofunqwords=len(set(tokenizedlcwords))\n",
    "    wordcov=noofwords/noofunqwords\n",
    "    large_uncommon_words = [word for word in tokenizedlcwords if word.isalpha()]\n",
    "    wordfreq = nltk.FreqDist(large_uncommon_words)\n",
    "    maxfreq=wordfreq.most_common(1)[0][0]\n",
    "    return noofwords,noofunqwords,int(wordcov//1),maxfreq\n",
    "if __name__ == '__main__':\n",
    "    textURL = input()\n",
    "\n",
    "    if not os.path.exists(os.getcwd() + \"/nltk_data\"):\n",
    "        with zipfile.ZipFile(\"nltk_data.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.getcwd())\n",
    "\n",
    "    noofwords, noofunqwords, wordcov, maxfreq = processRawText(textURL)\n",
    "    print(noofwords)\n",
    "    print(noofunqwords)\n",
    "    print(wordcov)\n",
    "    print(maxfreq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' is', ' cool']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "s = 'Python is cool!!!'\n",
    "print(re.findall(r'\\s\\w+\\b', s))\n",
    "pattern = r'\\w+'\n",
    "tokens2_3 = nltk.regexp_tokenize(\"Python is cool!!!\", pattern)\n",
    "len(tokens2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams\n",
    "* __Bigrams__ represent a set of two consecutive words appearing in a text.\n",
    "* __bigrams__ function is called on tokenized words, as shown in the following example, to obtain bigra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'is'),\n",
       " ('is', 'an'),\n",
       " ('an', 'awesome'),\n",
       " ('awesome', 'language'),\n",
       " ('language', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "s = 'Python is an awesome language.'\n",
    "tokens = nltk.word_tokenize(s)\n",
    "list(nltk.bigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Frequent Bigrams\n",
    "* Now let's find out three frequently occurring bigrams, present in english-kjv collection of genesis corpus.\n",
    "* Let's consider only those bigrams, whose words are having a length greater than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import genesis\n",
    "eng_tokens = genesis.words('english-kjv.txt')\n",
    "eng_bigrams = nltk.bigrams(eng_tokens)\n",
    "filtered_bigrams = [ (w1, w2) for w1, w2 in eng_bigrams if len(w1) >=5 and len(w2) >= 5 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Frequent Bigrams\n",
    "* After computing bi-grams, the following code computes frequency distribution and displays three most frequent bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('their', 'father'), 19), (('lived', 'after'), 16), (('seven', 'years'), 15)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_bifreq = nltk.FreqDist(filtered_bigrams)\n",
    "eng_bifreq.most_common(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Frequent After Words\n",
    "* Now let's see an example which determines the two most frequent words occurring after __living__ are determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('creature', 7), ('thing', 4)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import genesis\n",
    "eng_tokens = genesis.words('english-kjv.txt')\n",
    "eng_bigrams = nltk.bigrams(eng_tokens)\n",
    "eng_cfd = nltk.ConditionalFreqDist(eng_bigrams)\n",
    "eng_cfd['living'].most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Frequent Next Word\n",
    "* Now let's define a function named generate, which returns words occurring frequently after a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(cfd, word, n=5):\n",
    "    n_words = []\n",
    "    for i in range(n):\n",
    "      n_words.append(word)\n",
    "      word = cfd[word].max()\n",
    "    return n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Most Frequent Next Word\n",
    "* After defining the function generate, it is called with eng_cfd and living parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['living', 'creature', 'that', 'he', 'said']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(eng_cfd, 'living')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The output shows a word which occurs most frequently next to __living__ is __creature__.\n",
    "* Similarly __that__ occurs more frequently after __creature__ and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigrams\n",
    "Similar to __Bigrams__, __Trigrams__ refers to set of all three consecutive words appearing in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'is', 'an'),\n",
       " ('is', 'an', 'awesome'),\n",
       " ('an', 'awesome', 'language'),\n",
       " ('awesome', 'language', '.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Python is an awesome language.'\n",
    "tokens = nltk.word_tokenize(s)\n",
    "list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ngrams\n",
    "* __nltk__ also provides the function __ngrams__. It can be used to determine a set of all possible ___n consecutive words__ appearing in a text.\n",
    "* The following example displays a list of four consecutive words appearing in the text __s__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'is', 'an', 'awesome'),\n",
       " ('is', 'an', 'awesome', 'language'),\n",
       " ('an', 'awesome', 'language', '.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(tokens, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocations\n",
    "* A __collocation__ is a pair of words that occur together, very often.\n",
    "* For example, __red wine__ is a collocation.\n",
    "* One characteristic of a __collocation__ is that the words in it cannot be substituted with words having similar senses.\n",
    "* For example, the combination __maroon wine__ sounds odd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Collocations\n",
    "* Now let's see how to generate collocations from text with the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said unto; pray thee; thou shalt; thou hast; thy seed; years old;\n",
      "spake unto; thou art; LORD God; every living; God hath; begat sons;\n",
      "seven years; shalt thou; little ones; living creature; creeping thing;\n",
      "savoury meat; thirty years; every beast\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import genesis\n",
    "tokens = genesis.words('english-kjv.txt')\n",
    "gen_text = nltk.Text(tokens)\n",
    "gen_text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hands-on - NLP - Python - Bigrams and CollocationsNLP - Python - Bigrams and Collocations Define a function called `performBigramsAndCollocations`, which takes two parameters. The first parameter, `textcontent`, is a string, and the second parameter is `word`. The function definition code stub is given in the editor. Perform the following tasks: Tokenize all the words given in `textcontent`. The word should contain alphabets or numbers or underscore. Store the tokenized list of words in `tokenizedwords`. (Hint: Use regexp_tokenize)Convert all the words into lowercase. Store the result in `tokenizedwords`.Compute bigrams of the list `tokenizedwords`. Store the list of bigrams in `tokenizedwordsbigrams`.Filter only the bigrams from `tokenizedwordsbigrams`, where the words are not part of stopwords. Store the result in tokenizednonstopwordsbigrams. (Hint: Use stopwords corpora)Compute the conditional frequency of `tokenizednonstopwordsbigrams`, where condition and event refer to the words. Store the result in `cfd_bigrams`.Determine the three most frequent words occurring after the given `word`. Store the result in `mostfrequentwordafter`.Generate collocations from `tokenizedwords`. Store list of collocation words in `collocationwords`. Return `mostfrequentwordafter`, `collocationwords` variables from the function. Input Format for Custom TestingInput from stdin will be processed as follows and passed to the function. The first line contains a string `textcontent`. Text content is used to perform mostfrequentbigrams.The second line contains a string `word`, used to find the three most frequent words occurring after this word. Sample Case Sample InputSTDIN Function Parameters ----- ------------------- Thirty-five sports disciplines and four cultural activities.... → textcontent = 'Explain to me again why I shouldn't cheat?\" he asked....'sports → word = 'sports'Sample Output[('fans', 3), ('car', 3), ('disciplines', 1)]['sports car', 'sports fans']ExplanationThe first line displays all the three most frequent words occurring after the given `word`.The second line displays all collocation words for the given `textcontent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python3\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import zipfile\n",
    "os.environ['NLTK_DATA'] = os.getcwd() + \"/nltk_data\"\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "#\n",
    "# Complete the 'performBigramsAndCollocations' function below.\n",
    "#\n",
    "# The function accepts following parameters:\n",
    "#  1. STRING textcontent\n",
    "#  2. STRING word\n",
    "#\n",
    "\n",
    "def performBigramsAndCollocations(textcontent, word):\n",
    "    # Write your code here\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk import ConditionalFreqDist\n",
    "    tokenizedword = nltk.regexp_tokenize(textcontent, pattern = r'\\w*', gaps = False)\n",
    "    tokenizedwords = [x.lower() for x in tokenizedword if x != '']\n",
    "    tokenizedwordsbigrams=nltk.bigrams(tokenizedwords)\n",
    "    stop_words= stopwords.words('english')\n",
    "    tokenizednonstopwordsbigrams=[(w1,w2) for w1 , w2 in tokenizedwordsbigrams if (w1 not in stop_words and w2 not in stop_words)]\n",
    "    cfd_bigrams=nltk.ConditionalFreqDist(tokenizednonstopwordsbigrams)\n",
    "    mostfrequentwordafter=cfd_bigrams[word].most_common(3)\n",
    "    tokenizedwords = nltk.Text(tokenizedwords)\n",
    "    collocationword = tokenizedwords.collocation_list()\n",
    "    collocationwords=[]\n",
    "    for i in collocationword:\n",
    "        string=i[0]+\" \"+i[1]\n",
    "        collocationwords.append(string)\n",
    "            \n",
    "\n",
    "    return mostfrequentwordafter ,collocationwords\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    textcontent = input()\n",
    "\n",
    "    word = input()\n",
    "\n",
    "    if not os.path.exists(os.getcwd() + \"/nltk_data\"):\n",
    "        with zipfile.ZipFile(\"nltk_data.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.getcwd())\n",
    "\n",
    "    mostfrequentwordafter, collocationwords = performBigramsAndCollocations(textcontent, word)\n",
    "    print(sorted(mostfrequentwordafter, key=lambda element: (element[1], element[0]), reverse=True))\n",
    "    print(sorted(collocationwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'is', 'cool'),\n",
       " ('is', 'cool', '!'),\n",
       " ('cool', '!', '!'),\n",
       " ('!', '!', '!')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Python is cool!!!'\n",
    "tokens = nltk.word_tokenize(s)\n",
    "list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
