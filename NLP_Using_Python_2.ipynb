{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "* Stemming is a process of stripping affixes from words.\n",
    "* More often, you normalize text by converting all the words into lowercase. This will treat both words __The__ and __the__ as same.\n",
    "* With stemming, the words __playing__, __played__ and __play__ will be treated as single word, i.e. __play.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmers in nltk\n",
    "* __nltk__ comes with few stemmers.\n",
    "* The two widely used stemmers are __Porter__ and __Lancaster__ stemmers.\n",
    "* These stemmers have their own rules for string affixes.\n",
    "* The following example demonstrates stemming of word __builders__ using __PorterStemmer__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'builder'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import PorterStemmer\n",
    "porter = nltk.PorterStemmer()\n",
    "porter.stem('builders')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmers in nltk\n",
    "* Now let's see how to use __LancasterStemmer__ and stem the word __builders__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'build'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "lancaster.stem('builders')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lancaster Stemmer returns __build__ whereas Porter Stemmer returns __builder__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing with Stemming\n",
    "* Let's consider the text collection, __text1__.\n",
    "* Let's first determine the number of unique words present in original __text1__.\n",
    "* Then normalize the text by converting all the words into lower case and again determine the number of unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19317"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "len(set(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17231"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_words = [ word.lower() for word in text1] \n",
    "len(set(lc_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing with Stemming\n",
    "* Now let's further normalize text1 with Porter Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10927"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "p_stem_words = [porter.stem(word) for word in set(lc_words) ]\n",
    "len(set(p_stem_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above output shows that, after normalising with Porter Stemmer, the text1 collection has 10927 unique words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalising with Stemming\n",
    "* Now let's normalise with Lancaster stemmer and determine the unique words of __text1__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9036"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "l_stem_words = [lancaster.stem(word) for word in set(lc_words) ]\n",
    "len(set(l_stem_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Applying Lancaster Stemmer to text1 collection resulted in 9036 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Lemma\n",
    "* __Lemma__ is a lexical entry in a lexical resource such as word dictionary.\n",
    "* You can find multiple Lemma's with the same spelling. These are known as __homonyms__.\n",
    "* For example, consider the two Lemma's listed below, which are __homonyms__.<br>\n",
    "1. saw [verb] - Past tense of see<br>\n",
    "2. saw [noun] - Cutting instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "* __nltk__ comes with __WordNetLemmatizer__. This lemmatizer removes affixes only if the resulting word is found in lexical resource, __Wordnet__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15168"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "wnl_stem_words = [wnl.lemmatize(word) for word in set(lc_words) ]\n",
    "len(set(wnl_stem_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __WordNetLemmatizer__ is majorly used to build a vocabulary of words, which are valid Lemmas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hands-on - NLP - Python - Stemming and LemmatizationNLP - Python - Stemming and Lemmatization Define a function called `performStemAndLemma`, which takes a parameter. The first parameter, `textcontent`, is a string. The function definition code stub is given in the editor. Perform the following specified tasks: Tokenize all the words given in `textcontent`. The word should contain alphabets or numbers or underscore. Store the tokenized list of words in `tokenizedwords`. (Hint: Use regexp_tokenize)Convert all the words into lowercase from the unique set of `tokenizedwords`. Store the result into the variable `tokenizedwords`.Remove all the stop words from the `tokenizedwords`. Store the result into the variable `filteredwords`. (Hint: Use stopwords corpora)Stem each word present in `filteredwords` with PorterStemmer, and store the result in the list `porterstemmedwords`.Stem each word present in `filteredwords` with LancasterStemmer, and store the result in the list `lancasterstemmedwords`.Lemmatize each word present in `filteredwords` with WordNetLemmatizer, and store the result in the list `lemmatizedwords`. Return `porterstemmedwords`, `lancasterstemmedwords`, `lemmatizedwords` variables from the function. Input Format for Custom TestingInput from stdin will be processed as follows and passed to the function. The first line contains a string `textcontent`. Text content is used to perform stemming and lemmatization. Sample Case Sample InputSTDIN Function Parameters ----- ------------------- \"Explain to me again why I shouldn't cheat?\" he asked.... â†’ textcontent = 'Explain to me again why I shouldn't cheat?\" he asked....' Sample Output['ask', 'cheat', 'cheater', 'ever', 'explain', 'get', 'go', 'happi', 'know', 'lose', 'nobodi', 'other', 'punish', 'tell']['ask', 'che', 'che', 'ev', 'explain', 'get', 'go', 'happy', 'know', 'los', 'nobody', 'oth', 'pun', 'tel']['asked', 'cheat', 'cheater', 'ever', 'explain', 'get', 'go', 'happy', 'know', 'losing', 'nobody', 'others', 'punished', 'telling'] ExplanationThe first line displays all the Porter stemmed words for the given `textcontent`.The second line displays all the Lancaster stemmed words for the given `textcontent`.The third line displays all the Wordner lemmatized words for the given `textcontent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python3\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import zipfile\n",
    "os.environ['NLTK_DATA'] = os.getcwd()+\"/nltk_data\"\n",
    "import nltk\n",
    "\n",
    "#\n",
    "# Complete the 'performStemAndLemma' function below.\n",
    "#\n",
    "# The function accepts STRING textcontent as parameter.\n",
    "#\n",
    "\n",
    "def performStemAndLemma(textcontent):\n",
    "    # Write your code here\n",
    "    from nltk.corpus import stopwords\n",
    "    tokenizedword = nltk.regexp_tokenize(textcontent, pattern = r'\\w*', gaps = False)\n",
    "    #Step 2\n",
    "    tokenizedwords = [y for y in tokenizedword if y != '']\n",
    "    unique_tokenizedwords = set(tokenizedwords)\n",
    "    tokenizedwords = [x.lower() for x in unique_tokenizedwords if x != '']\n",
    "    #Step 3\n",
    "    #unique_tokenizedwords = set(tokenizedwords)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    filteredwords = []\n",
    "    for x in tokenizedwords:\n",
    "        if x not in stop_words:\n",
    "            filteredwords.append(x)\n",
    "    #Steps 4, 5 , 6\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    ls = nltk.stem.LancasterStemmer()\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    porterstemmedwords =[]\n",
    "    lancasterstemmedwords = []\n",
    "    lemmatizedwords = []\n",
    "    for x in filteredwords:\n",
    "        porterstemmedwords.append(ps.stem(x))\n",
    "        lancasterstemmedwords.append(ls.stem(x))\n",
    "        lemmatizedwords.append(wnl.lemmatize(x))\n",
    "    return porterstemmedwords, lancasterstemmedwords, lemmatizedwords\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    textcontent = input()\n",
    "\n",
    "    if not os.path.exists(os.getcwd() + \"/nltk_data\"):\n",
    "        with zipfile.ZipFile(\"nltk_data.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.getcwd())\n",
    "\n",
    "    porterstemmedwords, lancasterstemmedwords, lemmatizedwords = performStemAndLemma(textcontent)\n",
    "\n",
    "    print(sorted(porterstemmedwords))\n",
    "    print(sorted(lancasterstemmedwords))\n",
    "    print(sorted(lemmatizedwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lie\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "porter = nltk.PorterStemmer()\n",
    "print(porter.stem('lying'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print(lancaster.stem('basics'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print(wnl.lemmatize('women'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ceremoni\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "porter = nltk.PorterStemmer()\n",
    "print(porter.stem('ceremony'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "* The method of categorizing words into their parts of speech and then labeling them respectively is called __POS Tagging__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagger\n",
    "* A __POS Tagger__ processes a sequence of words and tags a part of speech to each word.\n",
    "* __pos_tag__ is the simplest tagger available in __nltk__.\n",
    "* The below example shows usage of __pos_tag__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NNP'), ('is', 'VBZ'), ('awesome', 'JJ'), ('.', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = 'Python is awesome.'\n",
    "words = nltk.word_tokenize(text)\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS_Tagger\n",
    "* The words Python, is and awesome are tagged to Proper Noun (NNP), Present Tense Verb (VB), and adjective (JJ) respectively.\n",
    "* You can read more about the pos tags with the below help command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To know about a specific tag like JJ, use the below-shown expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging Text\n",
    "* Constructing a list of tagged words from a string is possible.\n",
    "* A tagged word or token is represented in a tuple, having the word and the tag.\n",
    "* In the input text, each word and tag are separated by __/__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NN'), ('is', 'VB'), ('awesome', 'JJ'), ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Python/NN is/VB awesome/JJ ./.'\n",
    "[ nltk.tag.str2tuple(word) for word in text.split() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagged Corpora\n",
    "* Many of the text corpus available in __nltk__, are already tagged to their respective parts of speech.\n",
    "* __tagged_words__ method can be used to obtain tagged words of a corpus.\n",
    "* The following example fetches tagged words of __brown__ corpus and displays few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_tagged = brown.tagged_words()\n",
    "brown_tagged[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DefaultTagger\n",
    "* DefaultTagger assigns a specified tag to every word or token of given text.\n",
    "* An example of tagging NN tag to all words of a sentence, is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NN'), ('is', 'NN'), ('awesome', 'NN'), ('.', 'NN')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = 'Python is awesome.'\n",
    "words = nltk.word_tokenize(text)\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "default_tagger.tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lookup Tagger\n",
    "* You can define a custom tagger and use it to tag words present in any text.\n",
    "* The below-shown example defines a dictionary __defined_tags__, with three words and their respective tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = 'Python is awesome.'\n",
    "words = nltk.word_tokenize(text)\n",
    "defined_tags = {'is':'BEZ', 'over':'IN', 'who': 'WPS'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lookup Tagger\n",
    "* The example further defines a __UnigramTagger__ with the defined dictionary and uses it to predict tags of words in __text__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', None), ('is', 'BEZ'), ('awesome', None), ('.', None)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger = nltk.UnigramTagger(model=defined_tags)\n",
    "baseline_tagger.tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since the words Python and awesome are not found in defined_tags dictionary, they are tagged to None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Tagger\n",
    "* __UnigramTagger__ provides you the flexibility to create your taggers.\n",
    "* Unigram taggers are built based on statistical information. i.e., they tag each word or token to most likely tag for that particular word.\n",
    "* You can build a unigram tagger through a process known as __training__.\n",
    "* Then use the tagger to tag words in a test set and evaluate the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Tagger\n",
    "* Let's consider the tagged sentences of brown corpus collections, associated with government genre.\n",
    "* Let's also compute the training set size, i.e., 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3032\n",
      "2425\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories='government')\n",
    "brown_sents = brown.sents(categories='government')\n",
    "print(len(brown_sents))\n",
    "train_size = int(len(brown_sents)*0.8)\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7799495586380832"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = brown_tagged_sents[:train_size]\n",
    "test_sents = brown_tagged_sents[train_size:]\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "unigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __unigram_tagger__ is built by passing trained tagged sentences as argument to __UnigramTagger__.\n",
    "* The built __unigram_tagger__ is further evaluated with test sentences.\n",
    "* The following code snippet shows tagging words of a sentence, taken from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('first', 'OD'),\n",
       " ('step', 'NN'),\n",
       " ('is', 'BEZ'),\n",
       " ('a', 'AT'),\n",
       " ('comprehensive', 'JJ'),\n",
       " ('self', None),\n",
       " ('study', 'NN'),\n",
       " ('made', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('faculty', None),\n",
       " (',', ','),\n",
       " ('by', 'IN'),\n",
       " ('outside', 'IN'),\n",
       " ('consultants', 'NNS'),\n",
       " (',', ','),\n",
       " ('or', 'CC'),\n",
       " ('by', 'IN'),\n",
       " ('a', 'AT'),\n",
       " ('combination', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('two', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger.tag(brown_sents[3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. NLP - Python - POS TaggingNLP - Python - POS Tagging Define a function called `tagPOS` which takes three parameters. The first parameter, `textcontent`, is a string, the second parameter, `taggedtextcontent`, is also a string, and the third parameter, `defined_tags`, is a dictionary. The function definition code stub is given in the editor. Perform the following specified tasks: Tag the Part of Speech for the given `textcontent` words, store the result into the variable `nltk_pos_tags`. (Hint: Use pos_tag)Tag the Part of Speech for the given `taggedtextcontent` words using the Tagging Text method. Store the result into the variable `tagged_pos_tag`.Tag the Part of Speech for the given `textcontent` words and use `defined_tags` as a model in the Lookup Tagger method. Store the result into the variable `unigram_pos_tag`. Return `nltk_pos_tags`, `tagged_pos_tag`, `unigram_pos_tag` variables from the function. Input Format for Custom TestingInput from stdin will be processed as follows and passed to the function. The first line contains a string `textcontent`. Text content is used to tag Part of speech.The second line contains a string `taggedtextcontent`. The tagged text content is used to tag Part of speech, which contains the tag itself. Sample Case Sample InputSTDIN Function Parameters ----- ------------------- Python is awesome. â†’ textcontent = 'Python is awesome.'Python/NNP is/VBZ awesome/DT ./. â†’ taggedtextcontent = 'Python/NNP is/VBZ awesome/DT ./.' Sample Output[('Python', 'NNP'), ('is', 'VBZ'), ('awesome', 'JJ'), ('.', '.')][('Python', 'NNP'), ('is', 'VBZ'), ('awesome', 'DT'), ('.', '.')][('Python', None), ('is', 'VERB'), ('awesome', 'ADJ'), ('.', '.')] ExplanationThe first line displays POS tagged words using pos_tag for the given `textcontent`.The second line displays POS tagged words using the Tagging Text method for the given `taggedtextcontent`.The third line displays POS tagged words using the Lookup Tagger method for the given `textcontent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python3\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import zipfile\n",
    "os.environ['NLTK_DATA'] = os.getcwd() + \"/nltk_data\"\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Complete the 'tagPOS' function below.\n",
    "#\n",
    "# The function accepts following parameters:\n",
    "#  1. STRING textcontent\n",
    "#  2. STRING taggedtextcontent\n",
    "#\n",
    "\n",
    "def tagPOS(textcontent, taggedtextcontent, defined_tags):\n",
    "    # Write your code here\n",
    "    words = nltk.word_tokenize(textcontent)\n",
    "    nltk_pos_tags=nltk.pos_tag(words)\n",
    "    tagged_pos_tag=[ nltk.tag.str2tuple(word) for word in taggedtextcontent.split() ]\n",
    "    baseline_tagger = nltk.UnigramTagger(model=defined_tags)\n",
    "    unigram_pos_tag=baseline_tagger.tag(words)\n",
    "    return nltk_pos_tags,tagged_pos_tag,unigram_pos_tag\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    textcontent = input()\n",
    "\n",
    "    taggedtextcontent = input()\n",
    "    \n",
    "    if not os.path.exists(os.getcwd() + \"/nltk_data\"):\n",
    "        with zipfile.ZipFile(\"nltk_data.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.getcwd())\n",
    "\n",
    "    defined_tags = dict(brown.tagged_words(tagset='universal'))\n",
    "\n",
    "    nltk_pos_tags, tagged_pos_tag, unigram_pos_tag = tagPOS(textcontent, taggedtextcontent, defined_tags)\n",
    "\n",
    "    print(nltk_pos_tags)\n",
    "    print(tagged_pos_tag)\n",
    "    print(unigram_pos_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pow\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print(lancaster.stem('power'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
